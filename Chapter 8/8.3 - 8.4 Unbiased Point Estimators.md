# Unbiased Point Estimators

## Unbiased Point Estimators $\hat{\theta}$

| Target Parameter $\theta$ | Sample Size(s) | Point Estimator $\hat{\theta}$ | $E(\hat{\theta})$ |  $Var(\hat{\theta})$ | Standard Error $\sigma_{\hat{\theta}}$ |
| :--- |  :--- | :--- | :--- | :--- | :--- |
| $\mu$ | $n$ |  $\bar{Y} = \frac{1}{n} \sum\limits_{i=1}^{n} Y_{i}$ | $\mu$ |  $\frac{\sigma^{2}}{n}$ | $\frac{\sigma}{n}$ |
| $p$ | $n$ |  $\hat{p} = \frac{1}{n} \sum\limits_{i=1}^{n} Y_{i}s$ | $p$ |  $\frac{pq}{n}$ | $\sqrt{\frac{pq}{n}}$ |
| $\mu_{1} - \mu_{2}$ | $n_{1} \hspace{1mm} n_{2}$ |  $\bar{Y} _{1} - \bar{Y} _{2}= \frac{1}{n_{1}} \sum\limits_{i=1}^{n_{1}} Y_{i}^{(1)} - \frac{1}{n_{2}} \sum\limits_{i=1}^{n_{2}} Y_{i}^{(2)}$ | $\mu_{1} - \mu_{2}$ |  $\frac{\sigma^{2}_{1}}{n_{1}} + \frac{\sigma_{2}^{2}}{n_{2}}$ | $\sqrt{ \frac{ \sigma^{2}_{1} } { n_{1} } + \frac{ \sigma_{2}^{2}}{ n_{2} } } $ |
| $p_{1} - p_{2} $ | $n_{1} \hspace{1mm} n_{2}$ |  $\hat{p_{1}} - \hat{p_{2}} = \frac{1}{n_{1}} \sum\limits_{i=1}^{n_{1}} Y_{i}^{(1)} - \frac{1}{n_{2}} \sum\limits_{i=1}^{n_{2}} Y_{i}^{(2)}$ | $\hat{p} _{1} - \hat{p} _{2}$ |  $\frac{p_{1}q_{1}}{n_{1}} + \frac{p_{2}q_{2}}{n_{2}}$ | $\sqrt{\frac{p_{1}q_{1}}{n_{1}} + \frac{p_{2}q_{2}}{n_{2}}}$ |
    
> For 1 & 3, $Y_{1}, Y_{2}, ..., Y_{n}$ denotes a random sample of size $n$ from a population with mean $\mu$ and variance $\sigma^{2}$ \
> For 2 & 4, $Y_{1}, Y_{2}, ..., Y_{n}$ denotes a random sample of size $n$ from a population with Bernoulli distribution.\
> Examples above are unbiased point estimators for $\hat{\theta}$

## Unbiased Point Estimators for $\sigma^{2}$
* Let $Y_{1}, Y_{2}, ..., Y_{n}$ denotes a random sample of size $n$ from a population with mean $\mu$ and variance $\sigma^{2}$, then an **unbiased point estimator for $\sigma^{2}$** is
$$S^{2} = \frac{\sum\limits_{i=1}^{n} (Y_{i} - \bar{Y})^{2})}{n-1}$$

Short Proof

$$
\begin{align*} 
& A \equiv \sum\limits_{i=1}^{n} (Y_{i} - \bar{Y})^{2} = \sum\limits_{i=1}^{n} Y_{i}^{2} - n\bar{Y}^{2}  \\
& \Rightarrow E \bigg(\sum\limits_{i=1}^{n} Y_{i}^{2} \bigg) = \sum\limits_{i=1}^{n} E[Y_{i}^{2}] =  n \mu^{2} + n \sigma^{2} \\
& \Rightarrow E[n\bar{Y}^{2}] = n \bigg( Var(\bar{Y}) + E(\bar{Y})^{2} \bigg) = n (\frac{\sigma^{2}}{n} + \mu^{2}) = \sigma^{2} + n \mu^{2} \\
& E(A) = n \mu^{2} + n \sigma^{2} - ( \sigma^{2} + n \mu^{2}) = \sigma^{2} (n -1) \\ 
& \therefore E[S^{2}] = E \bigg[\frac{A}{n -1} \bigg] = \sigma^{2}
\end{align*}
$$
> Note: $S^{2}$ is an unbiased estimator for $\sigma^{2}$ 

> Note: S is not an unbiased estimator for $\sigma$

## Unbiasedness & Errors
* Unbiasedness $\Leftrightarrow E[\hat{\theta}] = \theta  \Leftrightarrow E[\hat{\theta} - \theta] = 0$

* Error of Estimation: measures the goodness of any point estimation in terms of distances between estimates and target paramter
    $$ \epsilon = | \hat{\theta} - \theta |$$
    > Absolute not neccessary.

    * Goal: $P(| \hat{\theta} - \theta | \leq b) = P(\epsilon \leq b) = q, \quad b > 0 \quad 0 < p < 1$
* Using Chebyshev's inequality we can obtain a convservative lower bound (Not ideal)
$$ P( |\hat{\theta} - \theta | < b) \geq 1- \frac{Var(\hat{\theta})}{b^{2}}$$
        
### Chebyshev's: Error Bound for $2\sigma_{\hat{\theta}}$
$$ P( |\hat{\theta} - \theta | < 2 \sigma_{\hat{\theta}}) =
   P(-2 \sigma_{\hat{\theta}} <\hat{\theta} - \theta  < 2 \sigma_{\hat{\theta}}) = 
   P( \theta -2 \sigma_{\hat{\theta}} <\hat{\theta}  < \theta - 2 \sigma_{\hat{\theta}})
   \geq 1- \frac{\sigma^{2}_{\hat{\theta}}}{(2\sigma_{\hat{\theta}})^{2}} \\
   \therefore
    P( \theta -2 \sigma_{\hat{\theta}} <\hat{\theta}  < \theta - 2 \sigma_{\hat{\theta}}) \geq \frac{3}{4}
   $$      


## Central Limit Theorem

Let $Y_{1}, Y_{2}, ..., Y_{n}$ be independent and identically distributed random variables with $E(Y_{i}) = \mu$ and $Var(Y_{i}) = \sigma^{2}$. 
If n is large enought then $\bar{Y} \sim \mathcal{N} (\mu, \frac{\sigma^{2}}{n})$

By the Central Limit 
## 2- standard error bound
| Interval     |   Definition  |    Additional Notes   |
| :---  |:---   | :--- |
| $\theta \pm 2 \sigma_{\hat{\theta}}$ |  $\hat{\theta}$ **falls into** intervals with $\approx$ 95%  |  <ul><li>Fixed Interval w/ Unknown Location</li><li>Provides a bound for $\hat{\theta}$ </li></ul> |
| $\hat{\theta} \pm 2 \sigma_{\hat{\theta}}$ |  $\theta$ **covers** intervals with $\approx$ 95%  |  <ul><li> Random Interval </li><li>Provides a confidence interval for $\theta$ </li></ul> |
> $\sigma_{\hat{\theta}}$ = Standard Error = Standard Deviation of the estimator $\hat{\theta}$

By the CLT, when $n$ is large enough $\bar{Y}$ becomes approximately normal then $P(\epsilon \leq 2\sigma_{\bar{Y}}) \approx .9544$



## Miscellaneous

| $\mu_{1} - \mu_{2}$ 

 $n_{1} \hspace{1mm} n_{2}$ 
 
 $\bar{Y} _{1} - \bar{Y} _{2}= \frac{1}{n_{1}} \sum\limits_{i=1}^{n_{1}} Y_{i}^{(1)} - \frac{1}{n_{2}} \sum\limits_{i=1}^{n_{2}} Y_{i}^{(2)}$ 
 
 $\mu_{1} - \mu_{2}$

 Part
 
 $\frac{\sigma^{2}_{1}}{n_{1}}$
 
 $ \frac{\sigma_{2}^{2}}{n_{2}}$
 
 Part
 
 $ \sqrt{ \frac{ \sigma^{2}_{1} } { n_{1} } }$  
 
 new line
 

 $  \frac{ \sigma_{2}^{2}}{ 1 } $
