# Rao-Blackwell Theorem and MVUE


## Definition: Minimum Variance Unbiased Estimator (MVUE)
$\hat{\theta}$ is said to be a (uniform) minimum variance unbaised estimator (MVUE) for $\theta$ if

1. $E(\hat{\theta}) = \theta$
2. If $E(\hat{\theta}^{\star})= \theta  \Rightarrow V(\hat{\theta}) \leq V(\hat{\theta}^{\star})$

> Note: MVUE does not always exist

## Rao-Blackwell Theorem
Let $\hat{\theta}$ be  an unbiased estimator for $\theta$ such that $V(\hat{\theta}) < \infty $. If $U$ is a sufficient statistic for $\theta$, define $\hat{\theta}^{\star} = E(\hat{\theta} | U)$.Then, for all $\theta$,

$$E(\hat{\theta}^{\star}) = \theta \quad V(\hat{\theta}^{\star}) \leq V(\hat{\theta})$$

### Proof
```math
E(\hat{\theta}^{\star}) = E(E(\theta | U)) = E(\hat{\theta}) = \theta \\

\begin{align*}
V(\hat{\theta}) &= V \bigg( E(\hat{\theta} | U) \bigg) + E \bigg(  V(\hat{\theta} | U)\bigg) \\
&=  {V(\hat{\theta}^{\star}) + \underbrace{E\bigg(V(\hat{\theta}\vert U)\bigg)}}_{\geq 0} 
\end{align*}

```


### Recall Helper Theorem 1: Law of Total Exepectation
 Let $Y_{1}$ and $Y_{2}$ be random variables. Then $E(Y_{1}) = E( E(Y_{1} | Y_{2}))$

```math
\begin{align*}
E(Y_{1}) &= \int\limits_{- \infty}^{\infty} \int\limits_{- \infty}^{\infty} y_{1} f(y_{1}, y_{2}) \,d y_{1} \,d y_{2} \\
&= \int\limits_{- \infty}^{\infty} \int\limits_{- \infty}^{\infty} y_{1} f(y_{1} | y_{2}) f(y_{2}) \,d y_{1} \,d y_{2} \\
&= \int\limits_{- \infty}^{\infty} \underbrace{\int\limits_{- \infty}^{\infty} y_{1} f(y_{1} | y_{2}) \,d y_{1} }_{E(Y_{1} | Y_{2} = y_{2})}  f(y_{2})  \,d y_{2} \\
&= \underbrace{\int\limits_{- \infty}^{\infty}{E(Y_{1} | Y_{2} = y_{2})}  f(y_{2})  \,d y_{2}}_{\text{Law of Total Expectation (Similar to Probability -ish)}} \\
&= E[E(Y_{1} | Y_{2})] \\
\end{align*}
```

### Recall Helper Theorem 2: 
Let $Y_{1}, Y_{2}$ denote random variables. Then $V(Y_{1}) = E\bigg(V(Y_{1} | V_{2})\bigg) + V\bigg(E(Y_{1} | Y_{2})\bigg)$
```math
\begin{align*}
V(Y_{1}) &= E(Y_{1}^{2}) - (E(Y_{1}))^{2} \\
&= E \bigg( E(Y_{1}^{2} | | Y_{2}) \bigg) - E(E(Y_{1} | Y_{2}))^{2} \\
&= \underbrace{E \bigg( E(Y_{1}^{2} | | Y_{2}) \bigg) + E\bigg( (E(Y_{1} | Y_{2}))^{2}\bigg)}_{E(V(Y_{1}| Y_{2}))} + \underbrace{ E\bigg( (E(Y_{1} | Y_{2}))^{2}\bigg)- E(E(Y_{1} | Y_{2}))^{2}}_{V(E(Y_{1} | Y_{3}))} \\
&= E\bigg(V(Y_{1} | V_{2})\bigg) + V\bigg(E(Y_{1} | Y_{2})\bigg)
\end{align*}
```

> Rao-Blackwell Theorem helps us the determine if the unbiased estimator is minimum



## Additional Information
https://web.eecs.umich.edu/~cscott/past_courses/eecs564w11/07_mvue.pdf
