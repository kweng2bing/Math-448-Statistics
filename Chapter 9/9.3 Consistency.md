# Consistency


> ## Recall: Limit Definition for Sequence: 
> A sequence $\{a_n\}$ is said to converge to a number $a$ if
> $$\forall \epsilon > 0, n \rightarrow \infty \Rightarrow | a_{n} -a| \leq \epsilon $$
> ### Example: $\frac{1}{n}$
> $$ \lim_{n \rightarrow \infty} \frac{1}{n} = 0 \equiv \forall \epsilon > 0, n \rightarrow \infty \Rightarrow  \bigg| \frac{1}{n} - 0 \bigg| \leq \epsilon $$
>> $\frac{1}{n}$ is fixed. The graph over $n$ is also fixed. For random variables, the graph is not fixed. Random variables can change over repeated trials. However, as $n \rightarrow \infty$ all the different graphs draw the same trend.

## Defintion of Converges in probability
A sequence of random variables $\{X\_{n} \}$ **converges in probability** (denoted as $X_{n} \xrightarrow{p}{} X$ ) towards the random variable $X$ if $\forall \epsilon > 0$

$$ \lim_{n \rightarrow \infty} P( |X_{n} - X| < \epsilon ) = 1 $$

### Corollary

$$\lim_{n \rightarrow \infty} P(|X_{n}- X| \geq \epsilon) = 0$$


## Theorem
An unbaised estimator $\hat{\theta}_{n}$ for $\theta$ is a consistent estimator if
$$\lim_{n \rightarrow \infty} (V(\hat{\theta}_{n}) = 0)$$

### Proof
$$P(| Y - \mu| >  k \sigma ) \leq \frac{1}{k^{2}} \quad \text{(Tchebysheff's theorem) }$$

Since $\hat{\theta}_{n}$ is an unbiased estimator (and a random variable), $E(\hat{\theta}_{n}) = \theta$ we have 

$$
\begin{align*}
&P(| \hat{\theta}_{n} - \theta | > k \sigma_{\hat{\theta}_{n}}) \leq \frac{1}{k^{2}} \\
&P(| \hat{\theta}_{n} - \theta | > \frac{\epsilon}{\sigma_{\hat{\theta}_{n}}} \sigma_{\hat{\theta}_{n}}) \leq \frac{1}{(\frac{\epsilon}{\sigma_{\hat{\theta}_{n}}})^{2}} \\
&P(| \hat{\theta}_{n} - \theta | > \epsilon ) \leq \frac{\sigma_{\hat{\theta}_{n}}^{2}}{\epsilon^{2}} \\
&P(| \hat{\theta}_{n} - \theta | > \epsilon ) \leq \lim_{n \rightarrow \infty}\frac{\sigma_{\hat{\theta}_{n}}^{2}}{\epsilon^{2}} = 0 \quad \text{Definition of Consistency }\\ 
\end{align*}
$$


## Properties 