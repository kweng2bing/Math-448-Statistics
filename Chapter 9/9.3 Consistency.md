# Consistency


> ## Recall: Limit Definition for Sequence: 
>A sequence $\{a_n\}$ is said to converge to a number $a$ if
> $$\forall \epsilon > 0, n \rightarrow \infty \Rightarrow | a_{n} -a| \leq \epsilon $$
>> ### Example: $\frac{1}{n}$
>> $$ \lim_{n \rightarrow \infty} \frac{1}{n} = 0 \equiv \forall \epsilon > 0, n \rightarrow \infty \Rightarrow  \bigg| \frac{1}{n} - 0 \bigg| \leq \epsilon $$
>* $\frac{1}{n}$ is fixed. The graph over $n$ is also fixed. 
>* For random variables, the graph is not fixed. Random variables can change over repeated trials. However, as $n \rightarrow \infty$ all the different graphs draw the same trend.

## Definition of Converges in probability
A sequence of random variables $\{X\_{1},X\_{2},...,X\_{n},... \}$ **converges in probability** (denoted as $X_{n} \xrightarrow{p}{} X$ ) towards the random variable $X$ if $\forall \epsilon > 0$

$$ \lim_{n \rightarrow \infty} P( |X_{n} - X| < \epsilon ) = 1 $$

### Corollary

$$\lim_{n \rightarrow \infty} P(|X_{n}- X| \geq \epsilon) = 0$$


## Definition of Consistency
An estimator  $\hat{\theta}\_{n}$ is a **consistent estimator** for $\theta$  if $\forall \epsilon > 0$

$$ \lim_{n \rightarrow \infty} P( |\hat{\theta}\_{n} - \theta| < \epsilon ) = 1 $$

* We say $\hat{\theta}\_{n}$ is consistent for $\theta$ iff $\hat{\theta}\_{n}$ is convergent in probablity to $\theta$
* Denoted as $\hat{\theta}\_{n} \xrightarrow{p}{} \theta$

* Consistency is a desirable(unrealistic) property of an estimator. It means that as the sample size goes to infinity, **we are almost 100% sure** the distance between $\hat{\theta}\_{n}$ and $\theta$ is infinitely small $(\forall \epsilon > 0)$
    * $n \rightarrow \infty$ not a fixed $n$
    * Concerns Mean and Variance$

## Theorem for Consistency

An **unbaised estimator** $\hat{\theta}\_{n}$ for $\theta$ is a consistent estimator if

$$\lim\_{n\rightarrow\infty}(V(\hat{\theta}\_{n})=0)$$

### Proof

Recall: Tchebysheff's theorem
$$P(| Y - \mu| >  k \sigma ) \leq \frac{1}{k^{2}}$$

Since $\hat{\theta}\_{n}$ is an unbiased estimator (and a random variable), $E(\hat{\theta}\_{n})=\theta$ we have 

```math
\begin{align*}
&P(| \hat{\theta}_{n} - \theta | > k \sigma_{\hat{\theta}_{n}}) \leq \frac{1}{k^{2}} \\
&P(| \hat{\theta}_{n} - \theta | > \frac{\epsilon}{\sigma_{\hat{\theta}_{n}}} \sigma_{\hat{\theta}_{n}}) \leq \frac{1}{(\frac{\epsilon}{\sigma_{\hat{\theta}_{n}}})^{2}} \\
&P(| \hat{\theta}_{n} - \theta | > \epsilon ) \leq \frac{\sigma_{\hat{\theta}_{n}}^{2}}{\epsilon^{2}} \\
&P(| \hat{\theta}_{n} - \theta | > \epsilon ) \leq \lim_{n \rightarrow \infty}\frac{\sigma_{\hat{\theta}_{n}}^{2}}{\epsilon^{2}} = 0 \quad \text{Definition of Consistency }\\ 
\end{align*}
```
### Example
* $\overline{Y}$ is a consistent estimator for $\mu$ since  $\lim\limits_{n \rightarrow \infty}Var(\overline{Y_{n}}) = \lim\limits_{n \rightarrow \infty} \frac{\sigma^{2}}{n} = 0$

> Note: Consistent estimators are almost like a constant for a large $n$
## Theorem for Consistency
An estimator is consistent iff $\lim\limits\_{n \rightarrow \infty} MSE(\hat{\theta}\_{n})=0 $ That is

$$
\begin{cases}
\lim\limits\_{n \rightarrow \infty}Bias(\hat{\theta}\_{n})=\lim\limits\_{n\rightarrow\infty}E(\hat{\theta}\_{n})-\theta=0\\
\lim\limits\_{n\rightarrow\infty}V(\hat{\theta}\_{n})=0\\
\end{cases}
$$

### Example

Consider $\hat{\theta}\_{n} = \frac{n}{n-1} \overline{Y}$.
* $\hat{\theta}\_{n}$ is not an unbiased estimator as $E[\hat{\theta}_{n}] = \frac{n}{n-1} \theta$ 

However, 
* $\lim\limits_{n \rightarrow \infty} Bias(\hat{\theta}\_{n})= \lim\limits_{n \rightarrow \infty} E [\hat{\theta}\_{n}] - \theta = \lim\limits_{n \rightarrow \infty}\frac{n}{n-1}\theta - \theta = \theta - \theta = 0$
* $\lim\limits_{n \rightarrow \infty} Var(\hat{\theta}\_{n}) = \lim\limits_{n \rightarrow \infty} \bigg(\frac{n}{n-1})^{2} \frac{\sigma^{2}}{n} \bigg) = 0$

$\therefore \hat{\theta}\_{n}$ is consistent 

## Additional Notes
* A biased estimator can be consistent
* An unbiased estimator can be inconsistent
* An unbiased consistent estimator can be less efficient than an unbiased unconsistent estimator up a to a certain point


## Properties of convergence in probability/consistency

$$ \hat{\theta}\_{n}\text{ is consistent for }\theta\Leftrightarrow\hat{\theta}\_{n} \xrightarrow{p}{} \theta$$

Let $\hat{\theta}\_{n}\xrightarrow{p}{}\theta$ and $\hat{\theta}^\prime\_{n} \rightarrow\theta^\prime$

1. $\hat{\theta}\_{n}+\hat{\theta}^\prime\_{n}\xrightarrow{p}{}\theta+\theta^\prime$
2. $\hat{\theta}\_{n}\times\hat{\theta}^\prime\_{n}\xrightarrow{p}{}\theta\times\theta^\prime$
3. $\frac{\hat{\theta}\_{n}}{\hat{\theta}^\prime\_{n}}\xrightarrow{p}{}\frac{\hat{\theta}}{\theta^\prime}$ only if $\theta^\prime\neq 0$
4. (**Continious Mapping Theorem**) For any continuous function, $g(u), g(\hat{\theta} _{n}) \xrightarrow{p}{} g(\theta)$
5. (**Continious Mapping Theorem**) For any continuous function $g(u,v),g(\hat{\theta} _{n},\hat{\theta}^{\prime}\_{n})\xrightarrow{p}{}g(\theta,\theta^{\prime})$,
6. For a sequence of numbers $a\_{n},a\_{n}\rightarrow a$ in the calculus sense implies that $a\_{n}\xrightarrow{p}{}a$
    * $a_{n}$ is viewed as a special random variable 
### $S^{2}$ is consistent to $\sigma^{2}$
We need to show 
$$P( | S_{n}^{2} - \sigma^{2} | \geq \epsilon) \rightarrow 0, \text{ as } n \rightarrow \infty \Leftrightarrow S_{n}^{2} \xrightarrow{p}{} \sigma^{2}$$

```math
\begin{align*}
S_{n}^{2} &= \frac{1}{n-1}\sum\limits_{i=1}^{n} (Y_{1}- \overline{Y}_{n})^{2} \\
&= \frac{1}{n-1}\sum\limits_{i=1}^{n} (Y^{2}_{1} - 2\overline{Y}Y_{i} + \overline{Y}^{2}_{n}) \\
&= \frac{1}{n-1}\sum\limits_{i=1}^{n} (Y^{2}_{1}) - 2\overline{Y}(n \overline{Y}) + n\overline{Y}^{2}_{n} \\
&= \frac{1}{n-1}\sum\limits_{i=1}^{n} (Y^{2}_{1}) - n\overline{Y}^{2}_{n} \\
&= \frac{n}{n-1} \bigg(\frac{1}{n}\sum\limits_{i=1}^{n} (Y^{2}_{1}) - \overline{Y}^{2}_{n} \bigg)\\
\end{align*}
```
Recall that testing
$$
Y\_{n}\xrightarrow{p}{}E(Y\_{n})\\
\overline{Y}\_{n}\xrightarrow{p}{}E(\overline{Y}\_{n})\\
\frac{1}{n}\sum\limits\_{i=1}^{n}Y\_{i}\xrightarrow{p}{}E(\overline{Y}\_{n})
$$

The end
