# Method of Maximum Likelihood

## Maximum Likelihood Estimator

* The Maximum Likelihood Estimator (MLE) is the value of $\theta$ which maximizes the likelihood, $L(\theta | y_{1} ... y_{n} )$ given the observed data $y_{1}, ... , y_{n}$
* MLE is a statistical method that estimates the parameters of a probability distribution based on observed data. It works by maximizing a likelihood function to make the observed data the most likely outcome under the assumed statistical model
> Yields good estimators and many times a MVUE

## Finding a Maximizer of a function
1. Find the likelihood function of the the parameter of interest given the data i.e. $L(\theta \,|\, y_{1},..., y_{n})$
2. $\ell (\theta) = \log(L(\theta))$
    * $\log(\cdot)$ is a monotonically increasing function
    * Since $L(\theta) > 0$, a maximum on $\log(L(\theta))$ is also a maximum on $L(\theta)$ 
3.  $\ell(\theta)$ is a continious function of $\theta$. We need to find $\theta^{\star}$ s.t. $\max( \ell(\theta^{\star}))$
    * Case 1 (No Boundary) Consider all the $\theta^{\star}$ s.t. 
        1. $f^{\prime}(x^{\star}) = 0 \quad$
        2. $f^{\prime\prime}(x^{\star}) < 0$ 
    * Case 2 (With Boundary) over $[a,b]
        1. $f^{\prime}(x^{\star}) = 0 \quad$
        2. $f^{\prime\prime}(x^{\star}) < 0$ 
        3. Consider $f(a), f(b)$
## Plug in Property
 If $\hat{\theta}_{MLE}$ is the MLE for $\theta$ and $g(\cdot )$ is a one-to-one function, then $g(\hat{\theta})$ is the fMLE for $\psi := g(\theta)$, i.e.

 $$ \hat{\psi}_{MLE} = g(\hat{\theta}_{MLE})$$


 ### Purpose
 * Shortcut for finding MLE 

 * If we know $\hat{\theta}_{MLE} \Rightarrow \theta$, and $  \psi =g(\theta)$, then we just need to find  $g(\hat{\theta}_{MLE}) \Rightarrow  g(\theta) = \psi$


## Example Questions

### Example 1
$Y \sim f(\theta)$
* Class of questions $\{ E(Y^{2}), Var(Y), E(Y), P(2< Y <5), P(Y=0) \}$

All function of $\theta$
* Becomes of function of $\theta$, and we are taking $\sum, \int$ resulting only a $\theta$
#### Example 1.5
Let $Y_{i} \sim Poisson(\lambda)$
* $f(y) = \frac{ e^{-\lambda} \lambda^{y}}{y!}$
* $\lambda_{MLE} \Rightarrow \overline{y}$

Solving for 
$$
\begin{align*}
&E(Y^{2}): E(Y^{2}) =\lambda^{2} + \lambda \Leftarrow  \overline{y}^{2} + \overline{y} \\
&Var(Y): Var(Y) = \lambda \Leftarrow \overline{y}\\
&E(Y): E(Y) = \lambda \Leftarrow  \overline{y} \\
&P(2 < Y < 5) : P(Y=3) + P(Y=4) = \frac{ e^{-\lambda} \lambda^{3}}{3!} +\frac{ e^{-\lambda} \lambda^{4}}{4!} \Leftarrow \frac{ e^{-\overline{y}} \overline{y}^{3}}{3!} +\frac{ e^{-\overline{y}} \overline{y}^{4}}{4!}\\
\end{align*}

$$
* $y$ becomes a constant

### Example 3
Problem: $Y_{i} \sim Bernoulli(p)$. Let $T \equiv \sum\limits_{i=1}^{n} Y_{i}$

Step 1: Likelihood of $p$ based on $\{ y_{i} \}$ is
$$ \begin{align*}
L(p) &= \prod\limits_{i=1}^{n} p^{y_{i}} (1-p)^{1-y_{i}} \\
&= p^{\sum\limits_{i=1}^{n} y_{i}} (1-p)^{n - \sum\limits_{i=1}^{n} y_{i}} \\
&= p^{t} (1-p)^{n-t} \\
\end{align*}
$$

Step 2: Set $\ell(p) = \ln(p)$ 
$$\ell(p) = t \log(p) + (n-t) \log(1-p) $$

Step 3a: Stationary points & Boundary Points
$$
\begin{align*}
\ell^{\prime} (p)  &= \frac{t}{p} - \frac{n-t}{1-p} \\
&\Rightarrow \frac{t}{p} - \frac{n-t}{1-p} = 0 \\
&\Rightarrow p = \frac{t}{n} \\
&\text{Boundary for p} \in [0,1] \\
&\ell(p) = \ell(0), \ell(1) = DNE \\ 
\end{align*}
$$
Step 3b: Check Second Derivative
$$
\ell^{\prime\prime}(p) =  -\frac{t}{p^{2}} - \frac{n-t}{(1-p)^{2}}\\
\ell^{\prime\prime} (p) \mid_{p = \frac{t}{n}} < 0
$$
Step 4: Conclusion
$$\therefore \hat{p} = \frac{T}{n}$$

* For $P(Y=0) \quad = {e^{-\lambda}} \Leftarrow e^{-\overline{y}}$


### Example 4
Problem: $Y_{i} \sim Poisson(\lambda)$. 
* $ p(y)\frac{ e^{-\lambda}\lambda^{y}}{y!}$

Step 1: Get Likelihood Function
$$
\begin{align*}
L(\lambda) &= \prod\limits_{i=1}^{n} \frac{ e^{-\lambda}\lambda^{y_{i}}}{y_{i}!}  \\
&= e^{-n\lambda} \frac{\lambda^{\sum y_{i}}}{\prod y_{i}!} \\
\end{align*}
$$

Step 2: Log Likelihood Function
$$
\ell(\lambda) = \sum y_{i} \ln(\lambda) - n \lambda - \ln(c)
$$

Step 3a: Stationary & Boundary Points 
$$\ell^{\prime}(\lambda)  = \sum y_{i}  \frac{1}{\lambda} - n = 0 \\
\Rightarrow  \lambda = \overline{y} \\
\text{Boundary points for } \lambda \in [0,\infty] \\
\ell(0) = DNE
$$

Step 3b. 
$$\ell^{\prime\prime} (\lambda) =- \sum y_{i}  \frac{1}{\lambda^{2}} \\
\ell^{\prime\prime} \vert_{\overline{y}} < 0$$

Step 4: Conclusion
$$\therefore \lambda_{MLE} = \overline{Y}$$
### Example 2
$$f(y) \sim (\theta + 1)y^{\theta} \quad 0 < y > 1, \quad \theta > -1$$
FInd the MLE

$$L(\theta) = \prod_{i=1}^{n} (\theta + 1)y^{\theta}_{i} = (\theta + 1)^{n} \prod_{i=1}^{n}y_{i}^{\theta} \\
\ln(L(\theta)) = n(\theta + 1) + \theta (\ln(\prod_{i=1}^{n}y_{i}) \\
\frac{\,d }{\,d \theta} = \frac{n}{\theta + 1} + (\ln(\prod_{i=1}^{n}y_{i}) \\
\frac{\,d^{2}}{\,d \theta^{2}} = -\frac{n}{(\theta + 1)^{2} }

$$

## Question

$Y \sim Binom(n,p)$ where $n$ is known and $p$ is to be estimated
* MLE for $p$ and $Var(Y) = np(1-p)$ 
* $\hat{p}_{MLE} $
* In here $p$ is a Bernoulli trial so $L(p)$ is with respect to Bernoulli instead of Binomial

Step 1: $$L(p) = \binom{n}{y} p^{y} (1-p)^{n-y}$$
Step 2: $$\log L(p) =  c_{1} + y ln(p) + (n-y) \ln(1-p)$$
Step 3: $$0 = \frac{d}{dp} \log L(p) = $$
Step 4: $$\frac{d^{2}}{dp^{2}} \log L(p) $$

$\hat{p} = \frac{Y}{n}$


