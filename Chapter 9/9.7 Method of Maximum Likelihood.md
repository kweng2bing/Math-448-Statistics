# Method of Maximum Likelihood

## Maximum Likelihood Estimator

* The Maximum Likelihood Estimator (MLE) is the value of $\theta$ which maximizes the likelihood, $L(\theta | y_{1} ... y_{n} )$ given the observed data $y_{1}, ... , y_{n}$
* MLE is a statistical method that estimates the parameters of a probability distribution based on observed data. It works by maximizing a likelihood function to make the observed data the most likely outcome under the assumed statistical model
> Yields good estimators and many times a MVUE
>> MLE is "asympototically MVUE"
## Finding a Maximizer of a function
1. Find the likelihood function of the the parameter of interest given the data i.e. $L(\theta \,|\, y_{1},..., y_{n})$
2. $\ell (\theta) = \log(L(\theta))$
    * $\log(\cdot)$ is a monotonically increasing function
    * Since $L(\theta) > 0$, a maximum on $\log(L(\theta))$ is also a maximum on $L(\theta)$ 
3.  $\ell(\theta)$ is a continious function of $\theta$. We need to find $\theta^{\star}$ s.t. $\max( \ell(\theta^{\star}))$
    * Case 1 (No Boundary) Consider all the $\theta^{\star}$ s.t. 
        1. $f^{\prime}(x^{\star}) = 0 \quad$
        2. $f^{\prime\prime}(x^{\star}) < 0$ 
    * Case 2 (With Boundary) over $[a,b]
        1. $f^{\prime}(x^{\star}) = 0 \quad$
        2. $f^{\prime\prime}(x^{\star}) < 0$ 
        3. Consider $f(a), f(b)$
## Plug in Property
 If $\hat{\theta}_{MLE}$ is the MLE for $\theta$ and $g(\cdot )$ is a one-to-one function, then $g(\hat{\theta})$ is the fMLE for $\psi := g(\theta)$, i.e.

 $$\hat{\psi}\_{MLE}=g(\hat{\theta}\_{MLE})$$


 ### Purpose
 * Shortcut for finding MLE 

 * If we know $\hat{\theta}\_{MLE}\Rightarrow\theta$,and $\psi=g(\theta)$, then we just need to find $g(\hat{\theta}\_{MLE})\Rightarrow g(\theta)=\psi$


## Large-sample properties of MLE (large $n$)
If $\hat{\theta}$ is the MLE, then for continuous $g, g(\hat{\theta})$ is the MLE for $\theta$.
Also,
* $g(\hat{\theta}) \xrightarrow{p}{} g(\theta)$ consistent
* $\frac{g(\hat{\theta}) - g(\theta)}{SE(g(\hat{\theta}))}$ converges to $\mathcal{N}(0,1)$ in distribution, where 
    * $$SE(g(\hat{\theta}))=\sqrt{\frac{\bigg[\frac{\partial g(\theta)}{g(\theta)}\bigg]^{2}}{n E\bigg(-\frac{\partial^{2}\log f(Y\vert\theta)}{\partial\theta^{2}}\bigg)}}$$

## Cramer-Rao Lower Bound
* $$\frac{\bigg[\frac{\partial g(\theta)}{g(\theta)}\bigg]^{2}}{nE \bigg(-\frac{\partial^{2} \log f(Y | \theta)}{\partial \theta^{2}}\bigg)}$$
    * $f(Y | \theta )$ is the pdf/pmf of random variable $Y$
* $\leq$ the varaicne of any unbaised estimator for $g(\theta)$ for a finite $n$
* If $g(\cdot)$ is the idenitity function, then $\frac{\partial g(\theta)}{\partial \theta} = 1$

## Example Questions

### Example 1
$Y \sim f(\theta)$
* Class of questions $\{ E(Y^{2}), Var(Y), E(Y), P(2< Y <5), P(Y=0) \}$

All function of $\theta$
* Becomes of function of $\theta$, and we are taking $\sum, \int$ resulting only a $\theta$
#### Example 1.5
Let $Y_{i} \sim Poisson(\lambda)$
* $f(y) = \frac{ e^{-\lambda} \lambda^{y}}{y!}$
* $\lambda_{MLE} \Rightarrow \overline{y}$

Solving for 
```math
\begin{align*}
&E(Y^{2}): E(Y^{2}) =\lambda^{2} + \lambda \Leftarrow  \overline{y}^{2} + \overline{y} \\
&Var(Y): Var(Y) = \lambda \Leftarrow \overline{y}\\
&E(Y): E(Y) = \lambda \Leftarrow  \overline{y} \\
&P(2 < Y < 5) : P(Y=3) + P(Y=4) = \frac{ e^{-\lambda} \lambda^{3}}{3!} +\frac{ e^{-\lambda} \lambda^{4}}{4!} \Leftarrow \frac{ e^{-\overline{y}} \overline{y}^{3}}{3!} +\frac{ e^{-\overline{y}} \overline{y}^{4}}{4!}\\
\end{align*}
```
> $y$ becomes a constant

### Example 3
Problem: $Y_{i} \sim Bernoulli(p)$. Let $T \equiv \sum\limits_{i=1}^{n} Y_{i}$

Step 1: Likelihood of $p$ based on $\{ y_{i} \}$ is
```math
\begin{align*}
L(p) &= \prod\limits_{i=1}^{n} p^{y_{i}} (1-p)^{1-y_{i}} \\
&= p^{\sum\limits_{i=1}^{n} y_{i}} (1-p)^{n - \sum\limits_{i=1}^{n} y_{i}} \\
&= p^{t} (1-p)^{n-t} \\
\end{align*}
```

Step 2: Set $\ell(p) = \ln(p)$ 
$$\ell(p) = t \log(p) + (n-t) \log(1-p) $$

Step 3a: Stationary points & Boundary Points
```math
\begin{align*}
\ell^{\prime} (p)  &= \frac{t}{p} - \frac{n-t}{1-p} \\
&\Rightarrow \frac{t}{p} - \frac{n-t}{1-p} = 0 \\
&\Rightarrow p = \frac{t}{n} \\
&\text{Boundary for p} \in [0,1] \\
&\ell(p) = \ell(0), \ell(1) = DNE \\ 
\end{align*}
```

Step 3b: Check Second Derivative
```math
\ell^{\prime\prime}(p) =  -\frac{t}{p^{2}} - \frac{n-t}{(1-p)^{2}}\\
\ell^{\prime\prime} (p) \mid_{p = \frac{t}{n}} < 0
```

Step 4: Conclusion
$$\therefore \hat{p} = \frac{T}{n}$$

* For $P(Y=0) \quad = {e^{-\lambda}} \Leftarrow e^{-\overline{y}}$

### Example 4
Problem: $Y_{i} \sim Poisson(\lambda)$. 
* $p(y)\frac{e^{-\lambda}\lambda^{y}}{y!}$

Step 1: Get Likelihood Function
```math
\begin{align*}
L(\lambda) &= \prod\limits_{i=1}^{n} \frac{ e^{-\lambda}\lambda^{y_{i}}}{y_{i}!}  \\
&= e^{-n\lambda} \frac{\lambda^{\sum y_{i}}}{\prod y_{i}!} \\
\end{align*}
```

Step 2: Log Likelihood Function
```math
\ell(\lambda) = \sum y_{i} \ln(\lambda) - n \lambda - \ln(c)
```

Step 3a: Stationary & Boundary Points 
```math
\ell^{\prime}(\lambda)  = \sum y_{i}  \frac{1}{\lambda} - n = 0 \\
\Rightarrow  \lambda = \overline{y} \\
\text{Boundary points for } \lambda \in [0,\infty] \\
\ell(0) = DNE
```

Step 3b: Check Second Derivative 
```math
\ell^{\prime\prime} (\lambda) =- \sum y_{i}  \frac{1}{\lambda^{2}} \\
\ell^{\prime\prime} \vert_{\overline{y}} < 0
```

Step 4: Conclusion
$$\therefore \lambda_{MLE} = \overline{Y}$$

> Technically $\lambda = \overline{y}$ is incorrect. The more correct way is $\hat{\lambda} = \overline{y} $

### Example 5
Problem: $Y_{i} \sim \mathcal{N}(\mu, \sigma^{2})$.

Step 1: Likelihood Function
```math
L(\mu, \sigma^{2}) = \prod\limits_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^{2}}} e^{\frac{-(y_{i} - \mu)^{2}}{2\sigma^{2}}} \\
 = (2\pi\sigma^{2})^{-\frac{n}{2}}e^{\frac{\sum (y_{i} - \mu)^{2}}{2 \sigma^{2}}}
```

Step 2: Loglikelihood Function
```math
\begin{align*}
\ell(\mu, \sigma^{2}) &= -\frac{n}{2}\ln(2\pi\sigma^{2}) - \frac{\sum (y_{i} - \mu)^{2}}{2 \sigma^{2}} \\
&= -\frac{n}{2} \ln(2 \pi) - \frac{n}{2} \ln(\sigma^{2}) - \frac{\sum (y_{i} - \mu)^{2}}{2 \sigma^{2}} \\
\end{align*}
```


Step 3a: Stationary Points and Boundary

$$
\begin{align*}
\frac{\,\partial \ell(\mu, \sigma^{2})}{\,\partial \mu} =& \frac{2\sum(y_{i} - \mu)}{2\sigma^{2}} = \frac{n(\overline{y} - \mu)}{\sigma^{2}} \\
&\frac{n(\overline{y} - \mu)}{\sigma^{2}} = 0 \Rightarrow \hat{\mu} = \overline{y} \\
% \hline
\frac{\,\partial \ell(\mu, \sigma^{2})}{\,\partial \sigma^{2}} =& -\frac{n}{2} \frac{1}{\sigma^{2}} + \frac{\sum (y_{i} - \mu)}{2} \frac{1}{(\sigma^{2})^{2}} \\
&-\frac{n}{2} \frac{1}{\sigma^{2}} + \frac{\sum (y_{i} - \mu)}{2} \frac{1}{(\sigma^{2})^{2}} = 0 \\
&\Rightarrow \hat\sigma^{2} = \frac{1}{n} \sum(y_{i} - \hat{\mu}) = \frac{1}{n} \sum\limits_{i=1}^{n} (y_{i} - \overline{y})^{2}
\end{align*}
$$
$$\text{Bounds for } \sigma^{2}, \mu := (-\infty, \infty)$$
> Note : Treat $\sigma^{2}$ as a single variable

Step 3b: Second Derivative Test and Hessian Matrix

```math
H (\mu, \sigma^{2})=
\begin{pmatrix}
\ell_{\mu\mu} & \ell_{\mu\sigma^{2}} \\
\ell_{\sigma^{2}\mu} & \ell_{\sigma^{2}\sigma^{2}} \\
\end{pmatrix}
=
\begin{pmatrix}
-\frac{n}{\sigma^{2}} & -\frac{n(\overline{y} - \mu)}{(\sigma^{2})^{2}} \\
- \sum(y_{i} - \mu) \frac{1}{(\sigma^{2})^{2}} & \frac{n}{2(\sigma^{2})^{2}} - \sum (y_{i} - \mu)^{2} \frac{1}{(\sigma^{2})^{3}} \\
\end{pmatrix}

\Rightarrow \det[H] > 0 
```
```math
\ell_{\mu\mu} \vert_{(\hat{\mu}, \hat{\sigma^{2}})} < 0 \land det(H(\hat{\mu}, \sigma^{2})) > 0
```

### Example 6
Problem: $$f(y) \sim (\theta + 1)y^{\theta} \quad 0 < y > 1, \quad \theta > -1$$
Find the MLE

Step 1: Likelihood Function

$$
L(\theta)=\prod_{i=1}^{n}(\theta + 1)y^{\theta}\_{i}=(\theta +1)^{n}\prod_{i=1}^{n}y\_{i}^{\theta}\\
$$

Step 2: Loglikelihood Function
$$
\ell(\theta)=\ln(L(\theta))=n(\theta+1)+\theta\ln(\prod\limits\_{i=1}^{n}y\_{i}) \\
$$

Step 3a: Stantionary Points & Boundary
```math
\begin{align*}
\frac{\,d }{\,d \theta} &= \frac{n}{\theta + 1} + (\ln(\prod_{i=1}^{n}y_{i}) \\
&\Rightarrow \hat{\theta} = \frac{n}{\ln (\prod\limits_{i=1}^{n} y_{i})} -1 \\

&\text{Boundary for } \theta:= [-1, \infty)\\
&\hat{\theta} = -1 ? \text{ Definitely not > than other stationary points} \\
\end{align*}
```

Step 3b: Check Second derivative
$$
\frac{\,d^{2}}{\,d\theta^{2}}=-\frac{n}{(\theta+1)^{2}}
$$

$$
\ell^{\prime\prime}(\theta)\vert\_{\hat{\theta}}<0
$$

Step 4: Conclusion

$$
\hat{\theta}=\frac{n}{\ln(\prod\limits_{i=1}^{n} y_{i})} -1\\
$$

### Example 7 (Cramer Rao Lower Bound Example)

$Y \sim Binom(n,p)$ where $n$ is known and $p$ is to be estimated
* MLE for $p$ and $Var(Y) = np(1-p)$ 
* $\hat{p}_{MLE} $
* In here $p$ is a Bernoulli trial so $L(p)$ is with respect to Bernoulli instead of Binomial

Step 1: $$L(p) = \binom{n}{y} p^{y} (1-p)^{n-y}$$
Step 2: $$\log L(p) =  c_{1} + y ln(p) + (n-y) \ln(1-p)$$
Step 3: $$0 = \frac{d}{dp} \log L(p) = $$
Step 4: $$\frac{d^{2}}{dp^{2}} \log L(p) $$

$\hat{p} = \frac{Y}{n}$


